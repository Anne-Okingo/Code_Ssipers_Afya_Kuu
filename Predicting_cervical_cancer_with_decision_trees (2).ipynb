{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGDEWIlDS6Cf"
   },
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "- `pandas` and `numpy` help us work with data.\n",
    "\n",
    "- `matplotlib` and `seaborn` are used to create charts and graphs.\n",
    "\n",
    "- From `scikit-learn`, we import tools to split data, build a random forest model, and check how well the model performs.\n",
    "\n",
    "- `compute_class_weight` helps us deal with class imbalance in the data.\n",
    "\n",
    "- `thefuzz` is used for fuzzy string matching—useful when text isn’t exactly the same.\n",
    "\n",
    "- `re` is Python’s tool for working with text patterns using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_EfJsu2hZqr"
   },
   "outputs": [],
   "source": [
    "# imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from thefuzz import process\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGDEWIlDS6Cf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "* [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) Random Forest Classifier documentation  \n",
    "* [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) train-test split documentation  \n",
    "* [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) compute class weight documentation  \n",
    "* [Scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html) metrics documentation (accuracy, confusion matrix, classification report)  \n",
    "* [Pandas documentation](https://pandas.pydata.org/docs/) for data manipulation  \n",
    "* [NumPy documentation](https://numpy.org/doc/) for numerical computing  \n",
    "* [Matplotlib](https://matplotlib.org/stable/index.html) official documentation for plotting  \n",
    "* [Seaborn](https://seaborn.pydata.org/) official documentation for statistical data visualization  \n",
    "* [Medium article](https://medium.com/@jaimejcheng/data-exploration-and-visualization-with-seaborn-pair-plots-40e6d3450f6d) for Seaborn pair plots  \n",
    "* [TheFuzz GitHub](https://github.com/seatgeek/thefuzz) for fuzzy string matching in Python  \n",
    "* [Medium article](https://towardsdatascience.com/fuzzy-string-matching-in-python-68f240d910fe) explaining fuzzy matching in Python  \n",
    "* [Hands-On Machine Learning book](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) for machine learning workflows and best practices  \n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) for accessing open datasets  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "## Step 2: Load and Prepare the Dataset\n",
    "\n",
    "- Load the dataset from the Excel file `Cervical Cancer Datasets_.xlsx`.\n",
    "- Save it as a CSV file named `cervical_cancer.csv` to make it easier to work with.\n",
    "- Load the CSV file into a Pandas DataFrame called `data` for further analysis.\n",
    "\n",
    "Note: We use `index=False` when saving to prevent Pandas from adding row numbers as a column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NChL47NVhXiv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 2. Load Excel and Save as CSV\n",
    "excel_path = 'Cervical Cancer Datasets_.xlsx'\n",
    "csv_path = 'cervical_cancer.csv'\n",
    "\n",
    "df = pd.read_excel(excel_path)\n",
    "df.to_csv(csv_path,index=False)\n",
    "\n",
    "# 3. Load CSV (use header=None only if the dataset has no headers)\n",
    "data = pd.read_csv(csv_path)  # Remove `header=None` if headers are present\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGDEWIlDS6Cf"
   },
   "source": [
    "### Flag Duplicates\n",
    "\n",
    "The `Patient ID` is treated as the unique identifier for each individual.\n",
    "\n",
    "We check for duplicate records by comparing all other columns **except `Patient ID`**. If rows have the same values across at least 80% of the fields, they are flagged as potential duplicates.\n",
    "\n",
    "A new column `is_duplicate` is added to the dataset to mark these cases.\n",
    "\n",
    "All flagged records are then exported to `flagged_duplicates.csv` for manual checking and review, ensuring no important data is accidentally removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag duplicates and export them for manual checking\n",
    "data['is_duplicate'] = data.duplicated(subset=df.columns.difference(['Patient ID']), keep=False)\n",
    "duplicates_df = data[data['is_duplicate'] == True]\n",
    "duplicates_df.to_csv('flagged_duplicates.csv', index=False)\n",
    "\n",
    "print(data.columns)\n",
    "print(\"\\n\")\n",
    "print(f\"The data set has ( Total rows: {len(data)}, Total columns: {len(data.columns)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "### Inspect and Verify Data Quality\n",
    "\n",
    "In this step, we perform a basic data audit:\n",
    "\n",
    "- Load the dataset from the CSV file.\n",
    "- Preview the data using `.head()` to understand its structure.\n",
    "- Use `.info()` to check data types and identify columns with missing values.\n",
    "- Count missing values per column to assess data completeness.\n",
    "- Print all unique values for each column to inspect potential inconsistencies or unexpected entries (e.g., typos in categorical fields).\n",
    "- Finally, we print the value counts for each column — including missing values — to evaluate how frequently each value appears. This helps identify:\n",
    "  - Class imbalance\n",
    "  - Invalid or rare values\n",
    "  - Columns that may require cleaning or transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "data = pd.read_csv(\"cervical_cancer.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Overview of column names, data types, and non-null counts\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Show unique values and value counts for each column\n",
    "for column in data.columns:\n",
    "    print(f\"--- Unique values for: {column} ---\")\n",
    "    print(data[column].unique())\n",
    "    print(f\"\\n--- Value Counts for: {column} ---\")\n",
    "    print(data[column].value_counts(dropna=False))\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) helps us deeply understand the structure, quality, and relationships within the data before applying any machine learning algorithms.\n",
    "\n",
    "### EDA Goals:\n",
    "- Identify variable types (categorical, numerical, binary)\n",
    "- Explore distributions of key features\n",
    "- Visualize relationships and potential predictors\n",
    "- Detect class imbalance in the target variable\n",
    "- Spot outliers or anomalies that may impact modeling\n",
    "- Support decisions for further data cleaning or transformation\n",
    "\n",
    "---\n",
    "\n",
    "### Feature & Data Type Overview\n",
    "\n",
    "We begin by checking the types of each column and the number of unique values. This helps us distinguish between categorical and numerical features and spot columns that need transformation or standardization.\n",
    "\n",
    "---\n",
    "\n",
    "### Known Data Quality Discrepancies\n",
    "\n",
    "During initial inspection, several inconsistencies and typos were identified:\n",
    "\n",
    "#### Column: `HPV Test Result`\n",
    "- `\"POSITIVE\\n\"` — extra newline character\n",
    "- `\"NEGAGTIVE\"` — misspelling of `\"NEGATIVE\"`\n",
    "\n",
    "#### Column: `Region`\n",
    "- Inconsistent casing: `\"Mombasa\"` vs `\"MOMBASA\"`, `\"Kitale\"` listed multiple times with trailing spaces\n",
    "- Spacing and capitalization differences\n",
    "\n",
    "#### Column: `Recommended Action`\n",
    "- Same recommendation repeated with:\n",
    "  - Extra spaces: `\"REPEAT PAP SMEAR IN 3 YEARS \"` vs `\"REPEAT PAP SMEAR IN 3 YEARS\"`\n",
    "  - Misspellings: `\"BIOSPY\"` instead of `\"BIOPSY\"`, `\"COLOSCOPY\"` instead of `\"COLPOSCOPY\"`\n",
    "  - Concatenation: `\"FORCOLPOSCOPY, CYTOLOGY THEN LASER THERAPY\"`\n",
    "\n",
    "#### Column: `Unnamed: 12`\n",
    "- Largely empty (mostly `NaN`) — candidate for removal.\n",
    "\n",
    "#### Column: `Insrance Covered`\n",
    "- Misspelled — should be renamed to `\"Insurance Covered\"`\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Outcome Class Imbalance\n",
    "\n",
    "We check whether the target variable (e.g., HPV Test Result or Pap Smear Result) is imbalanced.\n",
    "\n",
    "> Why it's important: Imbalanced datasets can mislead the model into favoring the dominant class, leading to poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Distribution of Numeric Features\n",
    "\n",
    "We explore numerical features like `Age`, `First Sexual Activity Age`, and `Sexual Partners` to check their distribution and spot outliers using histograms and boxplots.\n",
    "\n",
    "This step helps:\n",
    "- Identify skewness\n",
    "- Detect abnormal values\n",
    "- Guide transformation decisions\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Binary Features Overview\n",
    "\n",
    "Binary/categorical columns such as `Smoking Status`, `STDs History`, and `Insurance Covered` are analyzed to ensure:\n",
    "- Consistency (e.g., \"YES\"/\"NO\" vs \"Y\"/\"N\")\n",
    "- No missing or ambiguous values\n",
    "\n",
    "Visualizing their distribution gives insight into population characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Categorical Variable Exploration\n",
    "\n",
    "We examine values in columns like:\n",
    "- `Region` – Check patient spread across different geographical locations\n",
    "- `Screening Type Last` – Evaluate most commonly used screening methods\n",
    "- `Recommended Action` – Understand typical medical advice given after testing\n",
    "\n",
    "This helps us assess frequency, diversity, and inconsistencies in responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Correlation Matrix for Numerical Features\n",
    "\n",
    "A correlation matrix (heatmap) shows how numerical features relate to each other. This helps detect:\n",
    "- Redundant or highly correlated variables\n",
    "- Relationships that may influence the model\n",
    "\n",
    "Features explored include:\n",
    "- `Age`\n",
    "- `First Sexual Activity Age`\n",
    "- `Sexual Partners`\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Summary of Key Issues to Clean\n",
    "\n",
    "| Column                 | Issue Type             | Action Needed                     |\n",
    "|------------------------|------------------------|-----------------------------------|\n",
    "| `HPV Test Result`      | Typos, extra characters| Normalize strings                 |\n",
    "| `Region`               | Casing, spacing issues | Map to consistent labels          |\n",
    "| `Recommended Action`   | Misspellings, spacing  | Standardize text                  |\n",
    "| `Unnamed: 12`          | Mostly empty           | Drop this column                  |\n",
    "| `Insrance Covered`     | Misspelled             | Rename to `Insurance Covered`     |\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus: Scikit-learn Data Cleaning Pipeline (Optional)\n",
    "\n",
    "To make cleaning reusable and organized, we can define custom classes using `scikit-learn` transformers and compose them into a pipeline.\n",
    "\n",
    "Example concept:\n",
    "- Create a transformer to clean and standardize `HPV Test Result`\n",
    "- Chain transformers for cleaning multiple fields\n",
    "\n",
    "Using pipelines ensures our data transformations are:\n",
    "- Reproducible\n",
    "- Modular\n",
    "- Easily integrated into training workflows\n",
    "\n",
    "---\n",
    "\n",
    "### EDA Summary\n",
    "\n",
    "- Confirmed the presence of text inconsistencies, typos, and formatting issues in categorical columns.\n",
    "- Several numerical features show outliers or unusual values.\n",
    "- The target variable is imbalanced, which may require resampling.\n",
    "- These insights guide the next step: comprehensive data cleaning and transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "### 3.1: Initial Inspection of Raw Data\n",
    "\n",
    "In this step, we load the raw cervical cancer dataset and perform a quick inspection to identify data quality issues before cleaning. Specifically, we:\n",
    "\n",
    "- Suppress warning messages for a cleaner notebook display.\n",
    "- Load the dataset from a CSV file and print its shape to understand how many rows and columns it contains.\n",
    "- Print the column names to check for any typos or unnecessary fields.\n",
    "- Display unique values from key columns like `HPV Test Result`, `Region`, and a sample of `Recommended Action` values to identify inconsistencies (e.g., typos, inconsistent formatting, or unexpected categories).\n",
    "- Check for missing values in each column to determine which features may require imputation or removal during cleaning.\n",
    "\n",
    "This preliminary scan helps us understand what kind of cleaning and standardization will be necessary to prepare the data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the raw data\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_csv('cervical_cancer.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEFORE CLEANING - Data Quality Issues:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show original data quality issues\n",
    "print(\"1. Column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n2. Sample of problematic data:\")\n",
    "print(\"HPV Test Result unique values:\", df['HPV Test Result'].unique())\n",
    "print(\"Region unique values:\", df['Region'].unique())\n",
    "print(\"Recommended Action samples:\")\n",
    "for action in df['Recommended Action'].unique()[:5]:\n",
    "    print(f\"  - '{action}'\")\n",
    "\n",
    "print(\"\\n3. Missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "### 3.2: Data Cleaning and Standardization\n",
    "\n",
    "In this step, we clean and standardize the cervical cancer dataset to improve data quality, remove inconsistencies, and prepare it for further analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Cleaning Breakdown:\n",
    "\n",
    "1. **Create a Copy**  \n",
    "   - We create a copy of the original DataFrame (`clean_df = df.copy()`) to preserve the raw data for reference or backup.\n",
    "\n",
    "2. **Remove Unnecessary Columns**  \n",
    "   - We drop columns like `Unnamed: 12`, which are often artifacts of Excel exports and contain mostly `NaN` or irrelevant data.\n",
    "\n",
    "3. **Fix Column Name Typos**  \n",
    "   - Rename incorrectly spelled column headers.  \n",
    "     Example: `\"Insrance Covered\"` → `\"Insurance Covered\"`\n",
    "\n",
    "4. **Clean `HPV Test Result` Column**  \n",
    "   - Convert values to uppercase and strip whitespace.\n",
    "   - Replace typos and inconsistent formats like:\n",
    "     - `\"NEGAGTIVE\"` → `\"NEGATIVE\"`\n",
    "     - `\"POSITIVE\\n\"` → `\"POSITIVE\"`\n",
    "   - This standardization allows consistent analysis and visualization.\n",
    "\n",
    "5. **Clean and Standardize `Region` Names**  \n",
    "   - Remove inconsistencies caused by case sensitivity and trailing spaces.\n",
    "   - Use a mapping dictionary to unify variations:\n",
    "     - `\"MOMBASA\"`, `\"mombasa \"` → `\"Mombasa\"`\n",
    "     - `\"PUMWANI\"`, `\"Pumwani \"` → `\"Pumwani\"` etc.\n",
    "\n",
    "6. **Clean Binary Columns (Y/N Values)**  \n",
    "   - Columns like:\n",
    "     - `Pap Smear Result`, `Smoking Status`, `STDs History`, `Insurance Covered`\n",
    "   - We standardize these by:\n",
    "     - Converting to uppercase\n",
    "     - Stripping whitespace  \n",
    "   This ensures we don’t treat `\"Y\"`, `\"y \"`, and `\" Y\"` as different values.\n",
    "\n",
    "7. **Clean `Screening Type Last` Column**  \n",
    "   - Normalize string values using `upper()` and `strip()` to handle inconsistent formatting.\n",
    "\n",
    "8. **Handle Age Anomalies**  \n",
    "   - Convert `Age` to numeric and clip unrealistic values:\n",
    "     - Any age `< 10` or `> 100` is corrected to fall within that range.\n",
    "\n",
    "9. **Fix First Sexual Activity Age Conflicts**  \n",
    "   - Convert to numeric and check for logical errors:\n",
    "     - Drop rows where `First Sexual Activity Age > Age`.\n",
    "\n",
    "10. **Clean `Sexual Partners` Column**  \n",
    "    - Convert to numeric, drop rows with non-numeric or missing values.\n",
    "\n",
    "11. **Final String Cleanup**  \n",
    "    - Strip all string-type columns of whitespace for consistency.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "clean_df = df.copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING CLEANING TRANSFORMATIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# 1. Remove unnecessary columns\n",
    "print(\"1. Removing unnecessary columns...\")\n",
    "columns_to_drop = ['Unnamed: 12']\n",
    "for col in columns_to_drop:\n",
    "    if col in clean_df.columns:\n",
    "        clean_df.drop(columns=[col], inplace=True)\n",
    "        print(f\"   Dropped column: {col}\")\n",
    "        \n",
    "\n",
    "# 2. Fix column name typos\n",
    "print(\"\\n2. Fixing column name typos...\")\n",
    "column_renames = {\n",
    "    'Insrance Covered': 'Insurance Covered'\n",
    "}\n",
    "clean_df.rename(columns=column_renames, inplace=True)\n",
    "for old, new in column_renames.items():\n",
    "    print(f\"   Renamed '{old}' to '{new}'\")\n",
    "    \n",
    "\n",
    "# 3. Clean HPV Test Result\n",
    "print(\"\\n3. Cleaning HPV Test Result...\")\n",
    "print(f\"   Before: {clean_df['HPV Test Result'].unique()}\")\n",
    "\n",
    "clean_df['HPV Test Result'] = clean_df['HPV Test Result'].astype(str).str.upper().str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Fix the problematic characters - using raw strings to avoid escape issues\n",
    "hpv_replacements = {\n",
    "    'NEGAGTIVE': 'NEGATIVE',\n",
    "    'POSTIVE': 'POSITIVE',\n",
    "    'NEGATVIE': 'NEGATIVE',\n",
    "    'NEGATVE': 'NEGATIVE',\n",
    "    'NEGATIVE\\n':'NEGATIVE',\n",
    "    'NEGATIVE ':'NEGATIVE',\n",
    "    ' NEGATIVE ':'NEGATIVE',\n",
    "    ' POSITIVE ':'POSITIVE',\n",
    "    'POSITIVE\\n':'POSITIVE'\n",
    "}\n",
    "\n",
    "# Handle newline characters separately\n",
    "clean_df['HPV Test Result'] = clean_df['HPV Test Result'].str.replace('\\n', '', regex=False)\n",
    "clean_df['HPV Test Result'] = clean_df['HPV Test Result'].str.replace('\\\\n', '', regex=False)\n",
    "\n",
    "# Apply other replacements\n",
    "clean_df['HPV Test Result'] = clean_df['HPV Test Result'].replace(hpv_replacements)\n",
    "\n",
    "print(f\"   After: {clean_df['HPV Test Result'].unique()}\")\n",
    "\n",
    "\n",
    "# 4. Clean and standardize Region names\n",
    "print(\"\\n4. Cleaning and standardizing Region names...\")\n",
    "print(f\"   Before: {sorted(clean_df['Region'].unique())}\")\n",
    "\n",
    "# First, strip whitespace and normalize case\n",
    "clean_df['Region'] = clean_df['Region'].astype(str).str.strip()\n",
    "\n",
    "# Create comprehensive region mapping to handle all variations\n",
    "region_mapping = {\n",
    "    # Mombasa variations\n",
    "    'mombasa': 'Mombasa',\n",
    "    'MOMBASA': 'Mombasa',\n",
    "    'Mombasa': 'Mombasa',\n",
    "    'mombasa ': 'Mombasa',\n",
    "    'Mombasa ': 'Mombasa',\n",
    "    'MOMBASA ': 'Mombasa',\n",
    "\n",
    "\n",
    "      # Pumwani variations\n",
    "    'pumwani': 'Pumwani',\n",
    "    'PUMWANI': 'Pumwani',\n",
    "    'Pumwani': 'Pumwani',\n",
    "    'pumwani ': 'Pumwani',\n",
    "    'Pumwani ': 'Pumwani',\n",
    "    'PUMWANI ': 'Pumwani',\n",
    "    \n",
    "    # Embu variations\n",
    "    'embu': 'Embu',\n",
    "    'EMBU': 'Embu',\n",
    "    'Embu': 'Embu',\n",
    "    'embu ': 'Embu',\n",
    "    'Embu ': 'Embu',\n",
    "    'EMBU ': 'Embu',\n",
    "    \n",
    "    # Kakamega variations\n",
    "    'kakamega': 'Kakamega',\n",
    "    'KAKAMEGA': 'Kakamega',\n",
    "    'Kakamega': 'Kakamega',\n",
    "    'kakamega ': 'Kakamega',\n",
    "    'Kakamega ': 'Kakamega',\n",
    "    'KAKAMEGA ': 'Kakamega',\n",
    "    \n",
    "    # Machakos variations\n",
    "    'machakos': 'Machakos',\n",
    "    'MACHAKOS': 'Machakos',\n",
    "    'Machakos': 'Machakos',\n",
    "    'machakos ': 'Machakos',\n",
    "    'Machakos ': 'Machakos',\n",
    "    'MACHAKOS ': 'Machakos',\n",
    "    \n",
    "    # Nakuru variations\n",
    "    'nakuru': 'Nakuru',\n",
    "    'NAKURU': 'Nakuru',\n",
    "    'Nakuru': 'Nakuru',\n",
    "    'NAKURU ': 'Nakuru',\n",
    "    \n",
    "    # Moi variations\n",
    "    'moi': 'Moi',\n",
    "    'MOI': 'Moi',\n",
    "    'Moi': 'Moi',\n",
    "    'moi ': 'Moi',\n",
    "    'Moi ': 'Moi',\n",
    "    'MOI ': 'Moi',\n",
    "    \n",
    "  \n",
    "  \n",
    "    # Loitoktok variations\n",
    "    'loitoktok': 'Loitoktok',\n",
    "    'LOITOKTOK': 'Loitoktok',\n",
    "    'Loitoktok': 'Loitoktok',\n",
    "    'loitoktok ': 'Loitoktok',\n",
    "    'Loitoktok ': 'Loitoktok',\n",
    "    'LOITOKTOK ': 'Loitoktok',\n",
    "    \n",
    "    # Garissa variations\n",
    "    'garissa': 'Garissa',\n",
    "    'GARISSA': 'Garissa',\n",
    "    'Garissa': 'Garissa',\n",
    "    'garissa ': 'Garissa',\n",
    "    'Garissa ': 'Garissa',\n",
    "    'GARISSA ': 'Garissa',\n",
    "    \n",
    "    # Kericho variations\n",
    "    'kericho': 'Kericho',\n",
    "    'KERICHO': 'Kericho',\n",
    "    'Kericho': 'Kericho',\n",
    "    'kericho ': 'Kericho',\n",
    "    'Kericho ': 'Kericho',\n",
    "    'KERICHO ': 'Kericho',\n",
    "    \n",
    "    # Kitale variations\n",
    "    'kitale': 'Kitale',\n",
    "    'KITALE': 'Kitale',\n",
    "    'Kitale': 'Kitale',\n",
    "    'kitale ': 'Kitale',\n",
    "    'Kitale ': 'Kitale',\n",
    "    'KITALE ': 'Kitale'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Apply region mapping\n",
    "clean_df['Region'] = clean_df['Region'].replace(region_mapping)\n",
    "\n",
    "print(f\"   After: {sorted(clean_df['Region'].unique())}\")\n",
    "\n",
    "\n",
    "# 6. Clean binary columns (Y/N values)\n",
    "print(\"\\n6. Cleaning binary columns...\")\n",
    "binary_columns = ['Pap Smear Result', 'Smoking Status', 'STDs History', 'Insurance Covered']\n",
    "for col in binary_columns:\n",
    "    if col in clean_df.columns:\n",
    "        print(f\"   Cleaning {col}\")\n",
    "        print(f\"      Before: {clean_df[col].unique()}\")\n",
    "        clean_df[col] = clean_df[col].astype(str).str.upper().str.strip()\n",
    "        print(f\"      After: {clean_df[col].unique()}\")\n",
    "        \n",
    "\n",
    "# 7. Clean screening type\n",
    "print(\"\\n7. Cleaning Screening Type Last...\")\n",
    "if 'Screening Type Last' in clean_df.columns:\n",
    "    print(f\"   Before: {clean_df['Screening Type Last'].unique()}\")\n",
    "    clean_df['Screening Type Last'] = clean_df['Screening Type Last'].astype(str).str.upper().str.strip()\n",
    "    print(f\"   After: {clean_df['Screening Type Last'].unique()}\")\n",
    "    \n",
    "\n",
    "# 8. Handle age anomalies\n",
    "print(\"\\n8. Handling age anomalies...\")\n",
    "print(f\"   Age range before: {clean_df['Age'].min()} - {clean_df['Age'].max()}\")\n",
    "clean_df['Age'] = pd.to_numeric(clean_df['Age'], errors='coerce')\n",
    "# Check for unrealistic ages\n",
    "unrealistic_ages = clean_df[(clean_df['Age'] < 10) | (clean_df['Age'] > 100)]\n",
    "if len(unrealistic_ages) > 0:\n",
    "    print(f\"   Found {len(unrealistic_ages)} unrealistic ages\")\n",
    "clean_df['Age'] = clean_df['Age'].clip(lower=10, upper=100)\n",
    "print(f\"   Age range after: {clean_df['Age'].min()} - {clean_df['Age'].max()}\")\n",
    "\n",
    "# 9. Handle First Sexual Activity Age anomalies\n",
    "print(\"\\n9. Handling First Sexual Activity Age anomalies...\")\n",
    "print(f\"   First Sexual Activity Age range before: {clean_df['First Sexual Activity Age'].min()} - {clean_df['First Sexual Activity Age'].max()}\")\n",
    "clean_df['First Sexual Activity Age'] = pd.to_numeric(clean_df['First Sexual Activity Age'], errors='coerce')\n",
    "\n",
    "# Check for impossible values (first sexual activity age > current age)\n",
    "# Identify invalid rows\n",
    "impossible_ages = clean_df['First Sexual Activity Age'] > clean_df['Age']\n",
    "\n",
    "# Print how many invalid records were found\n",
    "if impossible_ages.any():\n",
    "    print(f\"   Found {impossible_ages.sum()} records where first sexual activity age > current age\")\n",
    "\n",
    "    # Drop those rows from the DataFrame\n",
    "    clean_df = clean_df[~impossible_ages]\n",
    "\n",
    "# 10. Handling Sexual Partners    \n",
    "\n",
    "print(\"\\n10. Handling Sexual Partners...\")\n",
    "\n",
    "#  Convert to numeric, non-numeric values become NaN\n",
    "clean_df['Sexual Partners'] = pd.to_numeric(clean_df['Sexual Partners'], errors='coerce')\n",
    "\n",
    "# Show range before dropping\n",
    "print(f\"   Sexual Partners range before cleanup: {clean_df['Sexual Partners'].min()} - {clean_df['Sexual Partners'].max()}\")\n",
    "\n",
    "# Drop rows where 'Sexual Partners' is NaN\n",
    "initial_count = len(clean_df)\n",
    "clean_df = clean_df.dropna(subset=['Sexual Partners'])\n",
    "dropped_count = initial_count - len(clean_df)\n",
    "\n",
    "print(f\"   Dropped {dropped_count} rows with missing or invalid 'Sexual Partners' values.\")\n",
    "\n",
    "# 11. Final cleanup - strip all string columns\n",
    "print(\"\\n11. Final cleanup...\")\n",
    "for col in clean_df.select_dtypes(include='object').columns:\n",
    "    clean_df[col] = clean_df[col].astype(str).str.strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLEANING COMPLETE - SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {clean_df.shape}\")\n",
    "print(f\"Columns after cleaning: {clean_df.columns.tolist()}\")\n",
    "\n",
    "# Show cleaned data quality\n",
    "print(\"\\nCleaned data quality:\")\n",
    "print(\"1. HPV Test Result values:\", clean_df['HPV Test Result'].unique())\n",
    "print(\"2. Regions:\", sorted(clean_df['Region'].unique()))\n",
    "print(\"3. Missing values after cleaning:\")\n",
    "print(clean_df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BOymaBxKZ"
   },
   "source": [
    "### 3.3: Final Validation and Saving of Cleaned Data\n",
    "\n",
    "After applying the data cleaning transformations, we perform a final check and save the cleaned dataset for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Steps Included:\n",
    "\n",
    "1. **Save Cleaned Dataset**  \n",
    "   - Export the cleaned `DataFrame` to a new CSV file named `'cervical_cancer_cleaned.csv'`.\n",
    "   - This ensures the cleaned version is preserved and can be reused for further analysis or modeling.\n",
    "\n",
    "2. **Preview Cleaned Data**  \n",
    "   - Display the first 5 rows of the cleaned dataset using `.head()` to get a sense of the cleaned structure and content.\n",
    "\n",
    "3. **Verify Key Columns**  \n",
    "   - Use a custom `show_unique_values()` function to display unique values for critical categorical features such as:\n",
    "     - `HPV Test Result`\n",
    "     - `Region`\n",
    "     - `Pap Smear Result`\n",
    "     - `Smoking Status`\n",
    "     - `STDs History`\n",
    "     - `Insurance Covered`\n",
    "     - `Screening Type Last`\n",
    "   - This helps confirm that cleaning steps were effective in removing typos, inconsistencies, and formatting issues.\n",
    "\n",
    "4. **Column-Wise Value Counts**  \n",
    "   - The `print_all_value_counts()` function prints value counts for all columns in the dataset (including `NaNs`).\n",
    "   - This helps detect any remaining anomalies and understand the distribution of each variable, which is critical for:\n",
    "     - Identifying imbalances\n",
    "     - Planning for encoding (e.g., one-hot, label)\n",
    "     - Feature engineering\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "clean_df.to_csv('cervical_cancer_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned data saved as 'cervical_cancer_cleaned.csv'\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "print(clean_df.head())\n",
    "\n",
    "# Show unique values for key columns to verify cleaning\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION OF CLEANED DATA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def show_unique_values(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            unique_vals = df[col].unique()\n",
    "            if len(unique_vals) <= 10:\n",
    "                print(f\"  Values: {unique_vals}\")\n",
    "            else:\n",
    "                print(f\"  Total unique values: {len(unique_vals)}\")\n",
    "                print(f\"  Sample: {unique_vals[:10]}\")\n",
    "\n",
    "verification_columns = ['HPV Test Result', 'Region', 'Pap Smear Result', 'Smoking Status', \n",
    "                       'STDs History', 'Insurance Covered', 'Screening Type Last']\n",
    "show_unique_values(clean_df, verification_columns)\n",
    "\n",
    "\n",
    "#Preview results\n",
    "def print_all_value_counts(df):\n",
    "    for column in df.columns:\n",
    "        print(f\"--- Value Counts for: {column} ---\")\n",
    "        print(df[column].value_counts(dropna=False))  # include NaNs in the count\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Print unique values to check validity & quality:\n",
    "print_all_value_counts(clean_df)\n",
    "\n",
    "print(\"\\n✅ Data cleaning completed successfully!\")\n",
    "print(\"✅ Ready for next step: Exploratory Data Analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list contains the final, correct values to use.\n",
    "canonical_values = [\n",
    "    \"REPEAT PAP SMEAR IN 3 YEARS\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY\",\n",
    "    \"FOR PAP SMEAR\",\n",
    "    \"FOR HPV VACCINE AND SEXUAL EDUCATION\",\n",
    "    \"FOR ANNUAL FOLLOW-UP AND PAP SMEAR IN 3 YEARS\",\n",
    "    \"REPEAT PAP SMEAR IN 3 YEARS AND FOR HPV VACCINE\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY (TAH NOT RECOMMENDED)\",\n",
    "    \"FOR BIOPSY AND CYTOLOGY (TAH NOT RECOMMENDED)\",\n",
    "    \"FOR HPV VACCINE, LIFESTYLE, AND SEXUAL EDUCATION\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, CYTOLOGY +/- TAH\",\n",
    "    \"FOR COLPOSCOPY, CYTOLOGY, THEN LASER THERAPY\",\n",
    "    \"FOR REPEAT HPV TESTING ANNUALLY AND PAP SMEAR IN 3 YEARS\",\n",
    "    \"FOR LASER THERAPY\"\n",
    "]\n",
    "\n",
    "# --- 2. Configuration for the Cleaner ---\n",
    "TYPO_MAP = {\n",
    "    'COLPOSOCPY': 'COLPOSCOPY', 'COLPOSCPY': 'COLPOSCOPY', 'COLOSCOPY': 'COLPOSCOPY',\n",
    "    'BIOSPY': 'BIOPSY', 'BIOSY': 'BIOPSY', 'ANUAL': 'ANNUAL', 'VACCINATION': 'VACCINE'\n",
    "}\n",
    "CRITICAL_KEYWORDS = {\n",
    "    'COLPOSCOPY', 'BIOPSY', 'CYTOLOGY', 'TAH', 'HPV', 'LASER', 'LIFESTYLE', 'ANNUALLY'\n",
    "}\n",
    "\n",
    "# --- 3. The Definitive Data Cleaning Function ---\n",
    "def clean_column_strict(df, column_to_clean, canonical_list, threshold=80):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame column using fuzzy matching combined with a STRICT keyword set equality rule.\n",
    "    \"\"\"\n",
    "    if column_to_clean not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_to_clean}' not found in the DataFrame.\")\n",
    "\n",
    "    def preprocess_and_get_keywords(text, typo_map):\n",
    "        if not isinstance(text, str): return \"\", set()\n",
    "        # Standardize to uppercase and correct typos\n",
    "        processed_text = text.upper()\n",
    "        for wrong, right in typo_map.items():\n",
    "            processed_text = re.sub(r'\\b' + wrong + r'\\b', right, processed_text)\n",
    "        \n",
    "        # Extract the set of critical keywords\n",
    "        keywords = {word for word in CRITICAL_KEYWORDS if word in processed_text}\n",
    "        return processed_text, keywords\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Pre-calculate keywords for the canonical list for efficiency\n",
    "    canonical_keywords = {val: preprocess_and_get_keywords(val, {})[1] for val in canonical_list}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        raw_string = row[column_to_clean]\n",
    "        \n",
    "        if not isinstance(raw_string, str) or not raw_string.strip():\n",
    "            cleaned_data.append((raw_string, 0, 'NO_DATA_PROVIDED'))\n",
    "            continue\n",
    "\n",
    "        processed_string, original_keywords = preprocess_and_get_keywords(raw_string, TYPO_MAP)\n",
    "        \n",
    "        # Get the top few potential matches instead of just one\n",
    "        top_matches = process.extract(processed_string, canonical_list, limit=5)\n",
    "\n",
    "        best_valid_match = None\n",
    "        \n",
    "        for potential_match, score in top_matches:\n",
    "            if score < threshold:\n",
    "                break # No need to check further if scores are too low\n",
    "\n",
    "            # The CRITICAL check: The keyword sets must be identical\n",
    "            if original_keywords == canonical_keywords[potential_match]:\n",
    "                best_valid_match = (potential_match, score)\n",
    "                break # Found the best possible valid match, stop searching\n",
    "\n",
    "        if best_valid_match:\n",
    "            final_value, final_score = best_valid_match\n",
    "            status = 'OK'\n",
    "        else:\n",
    "            final_value = raw_string\n",
    "            final_score = top_matches[0][1] if top_matches else 0 # Show score of best (but invalid) match\n",
    "            status = 'NEEDS MANUAL REVIEW'\n",
    "\n",
    "        cleaned_data.append((final_value, final_score, status))\n",
    "\n",
    "    result_df = pd.DataFrame(cleaned_data, index=df.index, columns=[f'{column_to_clean}_Cleaned', f'{column_to_clean}_Confidence', f'{column_to_clean}_Status'])\n",
    "    return df.join(result_df)\n",
    "\n",
    "# main_df = pd.DataFrame(data, columns=['Recommended Action'])\n",
    "\n",
    "\n",
    "# 2. Call the function to clean the specified column\n",
    "# You provide your DataFrame, the column name, the list of correct values, and the confidence threshold.\n",
    "final_df = clean_column_strict(\n",
    "    df=clean_df,\n",
    "    column_to_clean='Recommended Action',\n",
    "    canonical_list=canonical_values,\n",
    "    threshold=80\n",
    ")\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"--- Full Data Cleaning Results ---\")\n",
    "# Use .to_string() to ensure all columns are displayed without truncation\n",
    "#print(final_df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Display only the rows that need human attention\n",
    "print(\"--- Actions Flagged for Manual Review (Confidence < 80%) ---\")\n",
    "manual_review_df = final_df[final_df['Recommended Action_Status'] == 'NEEDS MANUAL REVIEW']\n",
    "\n",
    "if manual_review_df.empty:\n",
    "    print(\"No actions require manual review. All items met the 80% confidence threshold.\")\n",
    "else:\n",
    "    print(manual_review_df[['Recommended Action', 'Recommended Action_Cleaned', 'Recommended Action_Confidence']].to_string())\n",
    "\n",
    "\n",
    "print(f\"   After: {final_df['Recommended Action_Cleaned'].unique()}\")\n",
    "final_df\n",
    "final_df.to_excel(\"final_cleaned.xlsx\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "# Data Visualization\n",
    "In this section, we explore the dataset visually to understand distributions, relationships, and trends across various features. This is a crucial step in identifying:\n",
    "\n",
    "- Imbalanced classes\n",
    "- Outliers\n",
    "- Skewed data\n",
    "- Correlations between features\n",
    "\n",
    "We use common plotting tools like:\n",
    "- **Histograms** to explore the distribution of numeric values\n",
    "- **Box plots** to detect outliers\n",
    "- **Count plots** to analyze category frequency\n",
    "- **Heatmaps** to examine feature correlation\n",
    "\n",
    "These insights guide decisions for data preprocessing and feature selection before training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned Excel file\n",
    "final_df = pd.read_excel(\"cleaned_data.xlsx\")\n",
    "\n",
    "# Preview the data\n",
    "print(\" Data Preview \")\n",
    "print(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns (int or float)\n",
    "numerical_cols = final_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_cols = final_df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of Recommended Actions (from original column name)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(\n",
    "    data=final_df,\n",
    "    y='Recommended Action',\n",
    "    order=final_df['Recommended Action'].value_counts().index,\n",
    "    palette='Set1'\n",
    "    # palette='tab20'\n",
    ")\n",
    "plt.title(\"Distribution of Recommended Actions\")\n",
    "plt.xlabel(\"Number of Patients\")\n",
    "plt.ylabel(\"Recommended Action\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Summary of Categorical Columns\")\n",
    "print(final_df[categorical_cols].describe())\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     print(f\"\\n--- {col} ---\")\n",
    "#     print(final_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot each categorical column EXCEPT Recommended Action\n",
    "for col in categorical_cols:\n",
    "    if col != 'Recommended Action':\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.countplot(data=final_df, x=col, order=final_df[col].value_counts().index, palette='Set1')\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Handle 'Recommended Action' separately (horizontally)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(\n",
    "    data=final_df,\n",
    "    y='Recommended Action',\n",
    "    order=final_df['Recommended Action'].value_counts().index,\n",
    "    palette='Set1'\n",
    ")\n",
    "plt.title(\"Distribution of Recommended Actions\")\n",
    "plt.xlabel(\"Number of Patients\")\n",
    "plt.ylabel(\"Recommended Action\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for Numerical Features\n",
    "numerical_cols = ['Age', 'Sexual Partners', 'First Sexual Activity Age']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data=final_df, x=col, kde=True, bins=30, color='dodgerblue')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(final_df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Between Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Summary Stats for Numeric Features\n",
    "print(\"\\n Summary of Numerical Columns \")\n",
    "print(final_df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "## Step 4: Data Processing\n",
    "\n",
    "\n",
    "**Goal:** Clean the raw dataset to ensure consistency, standardization, and readiness for analysis or modeling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "### Standardizing Target Column Values (Recommendation)\n",
    "\n",
    "Ensure the target column (`Recommendation`) is standardized and balanced to prevent model bias toward specific values.\n",
    "\n",
    "---\n",
    "\n",
    "- **Define canonical values**  \n",
    "  Create a clean, consistent set of allowed values for the target column based on domain understanding.\n",
    "\n",
    "- **Apply fuzzy matching with confidence scoring**  \n",
    "  Use string similarity metrics (e.g., Levenshtein distance) to match variations of the values to their canonical forms.\n",
    "\n",
    "- **Flag low-confidence matches for manual review**  \n",
    "  If a value's match confidence is below a defined threshold (e.g., <80%), it is excluded from automatic cleaning and sent for manual validation.\n",
    "\n",
    "---\n",
    "\n",
    "> Standardizing target values is a critical step to ensure the model learns from meaningful, consistent labels and avoids biased predictions due to inconsistent or noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This list contains the final, correct values to use.\n",
    "canonical_values = [\n",
    "    \"REPEAT PAP SMEAR IN 3 YEARS\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY\",\n",
    "    \"FOR PAP SMEAR\",\n",
    "    \"FOR HPV VACCINE AND SEXUAL EDUCATION\",\n",
    "    \"FOR ANNUAL FOLLOW-UP AND PAP SMEAR IN 3 YEARS\",\n",
    "    \"REPEAT PAP SMEAR IN 3 YEARS AND FOR HPV VACCINE\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY (TAH NOT RECOMMENDED)\",\n",
    "    \"FOR BIOPSY AND CYTOLOGY (TAH NOT RECOMMENDED)\",\n",
    "    \"FOR HPV VACCINE, LIFESTYLE, AND SEXUAL EDUCATION\",\n",
    "    \"FOR COLPOSCOPY, BIOPSY, CYTOLOGY +/- TAH\",\n",
    "    \"FOR COLPOSCOPY, CYTOLOGY, THEN LASER THERAPY\",\n",
    "    \"FOR REPEAT HPV TESTING ANNUALLY AND PAP SMEAR IN 3 YEARS\",\n",
    "    \"FOR LASER THERAPY\"\n",
    "]\n",
    "\n",
    "# --- 2. Configuration for the Cleaner ---\n",
    "TYPO_MAP = {\n",
    "    'COLPOSOCPY': 'COLPOSCOPY', 'COLPOSCPY': 'COLPOSCOPY', 'COLOSCOPY': 'COLPOSCOPY',\n",
    "    'BIOSPY': 'BIOPSY', 'BIOSY': 'BIOPSY', 'ANUAL': 'ANNUAL', 'VACCINATION': 'VACCINE'\n",
    "}\n",
    "CRITICAL_KEYWORDS = {\n",
    "    'COLPOSCOPY', 'BIOPSY', 'CYTOLOGY', 'TAH', 'HPV', 'LASER', 'LIFESTYLE', 'ANNUALLY'\n",
    "}\n",
    "\n",
    "# --- 3. The Definitive Data Cleaning Function ---\n",
    "def clean_column_strict(df, column_to_clean, canonical_list, threshold=80):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame column using fuzzy matching combined with a STRICT keyword set equality rule.\n",
    "    \"\"\"\n",
    "    if column_to_clean not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_to_clean}' not found in the DataFrame.\")\n",
    "\n",
    "    def preprocess_and_get_keywords(text, typo_map):\n",
    "        if not isinstance(text, str): return \"\", set()\n",
    "        # Standardize to uppercase and correct typos\n",
    "        processed_text = text.upper()\n",
    "        for wrong, right in typo_map.items():\n",
    "            processed_text = re.sub(r'\\b' + wrong + r'\\b', right, processed_text)\n",
    "        \n",
    "        # Extract the set of critical keywords\n",
    "        keywords = {word for word in CRITICAL_KEYWORDS if word in processed_text}\n",
    "        return processed_text, keywords\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Pre-calculate keywords for the canonical list for efficiency\n",
    "    canonical_keywords = {val: preprocess_and_get_keywords(val, {})[1] for val in canonical_list}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        raw_string = row[column_to_clean]\n",
    "        \n",
    "        if not isinstance(raw_string, str) or not raw_string.strip():\n",
    "            cleaned_data.append((raw_string, 0, 'NO_DATA_PROVIDED'))\n",
    "            continue\n",
    "\n",
    "        processed_string, original_keywords = preprocess_and_get_keywords(raw_string, TYPO_MAP)\n",
    "        \n",
    "        # Get the top few potential matches instead of just one\n",
    "        top_matches = process.extract(processed_string, canonical_list, limit=5)\n",
    "\n",
    "        best_valid_match = None\n",
    "        \n",
    "        for potential_match, score in top_matches:\n",
    "            if score < threshold:\n",
    "                break # No need to check further if scores are too low\n",
    "\n",
    "            # The CRITICAL check: The keyword sets must be identical\n",
    "            if original_keywords == canonical_keywords[potential_match]:\n",
    "                best_valid_match = (potential_match, score)\n",
    "                break # Found the best possible valid match, stop searching\n",
    "\n",
    "        if best_valid_match:\n",
    "            final_value, final_score = best_valid_match\n",
    "            status = 'OK'\n",
    "        else:\n",
    "            final_value = raw_string\n",
    "            final_score = top_matches[0][1] if top_matches else 0 # Show score of best (but invalid) match\n",
    "            status = 'NEEDS MANUAL REVIEW'\n",
    "\n",
    "        cleaned_data.append((final_value, final_score, status))\n",
    "\n",
    "    result_df = pd.DataFrame(cleaned_data, index=df.index, columns=[f'{column_to_clean}_Cleaned', f'{column_to_clean}_Confidence', f'{column_to_clean}_Status'])\n",
    "    return df.join(result_df)\n",
    "\n",
    "# main_df = pd.DataFrame(data, columns=['Recommended Action'])\n",
    "\n",
    "\n",
    "# 2. Call the function to clean the specified column\n",
    "# You provide your DataFrame, the column name, the list of correct values, and the confidence threshold.\n",
    "final_df = clean_column_strict(\n",
    "    df=clean_df,\n",
    "    column_to_clean='Recommended Action',\n",
    "    canonical_list=canonical_values,\n",
    "    threshold=80\n",
    ")\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"--- Full Data Cleaning Results ---\")\n",
    "# Use .to_string() to ensure all columns are displayed without truncation\n",
    "#print(final_df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Display only the rows that need human attention\n",
    "print(\"--- Actions Flagged for Manual Review (Confidence < 80%) ---\")\n",
    "manual_review_df = final_df[final_df['Recommended Action_Status'] == 'NEEDS MANUAL REVIEW']\n",
    "\n",
    "if manual_review_df.empty:\n",
    "    print(\"No actions require manual review. All items met the 80% confidence threshold.\")\n",
    "else:\n",
    "    print(manual_review_df[['Recommended Action', 'Recommended Action_Cleaned', 'Recommended Action_Confidence']].to_string())\n",
    "\n",
    "\n",
    "print(f\"   After: {final_df['Recommended Action_Cleaned'].unique()}\")\n",
    "final_df\n",
    "final_df.to_excel(\"final_cleaned.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the file\n",
    "file_path = 'final_cleaned.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"1. Removing unnecessary columns...\")\n",
    "columns_to_drop = ['Recommended Action','Recommended Action_Confidence','Recommended Action_Status','Patient ID']\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "df.to_excel('data_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "\n",
    "# STEP 2: DATA SPLITTING\n",
    "## Goal:  Split the dataset to get the portion that goes into training and the portion that goes into testing\n",
    "\n",
    "We'll do:\n",
    "\n",
    "* Pre-split Fix.We notice that in the dataset,some critical columns have only one instance of it.Here we will use simple oversampling by manually duplicating these classes with only one instance.\n",
    "* stratified splitting on the data to ensure equal amount of percentage goes into both training and testing.\n",
    "* hot-encode remaining columns\n",
    "  # STEP 2B\n",
    "  ## Goal :Define class weights based on domain knowledge(Medical) to be fed into the data for use during training .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Your Data ---\n",
    "# Let's create a sample DataFrame that mimics your situation\n",
    "\n",
    "data = pd.read_excel('data_final.xlsx')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. DEFINE TARGET and SEPARATE X and y ---\n",
    "target_column = 'Recommended Action_Cleaned'\n",
    "y = df[target_column]\n",
    "X = df.drop(columns=[target_column])\n",
    "\n",
    "\n",
    "# --- 3. PRE-SPLIT STEP: LABEL ENCODE THE TARGET COLUMN ---\n",
    "# This is required so that the 'stratify' parameter can work with numeric labels.\n",
    "print(\"\\n--- 3. Label Encoding Target Column (y) ---\")\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"Target column converted to numeric labels.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# --- 4. PRE-SPLIT STEP: HANDLE SINGLE-INSTANCE CLASSES for STRATIFICATION ---\n",
    "# Stratify needs at least 2 members per class. We duplicate the singletons.\n",
    "print(\"\\n--- 4. Checking for and Fixing Single-Instance Classes ---\")\n",
    "class_counts = pd.Series(y_encoded).value_counts()\n",
    "single_instance_classes = class_counts[class_counts < 2].index\n",
    "\n",
    "X_fixed = X.copy()\n",
    "y_fixed = y_encoded.copy()\n",
    "\n",
    "if len(single_instance_classes) > 0:\n",
    "    print(f\"Found {len(single_instance_classes)} classes with only one sample. Duplicating them...\")\n",
    "    \n",
    "    indices_to_duplicate = pd.Series(y_encoded).isin(single_instance_classes)\n",
    "    \n",
    "    X_to_duplicate = X[indices_to_duplicate]\n",
    "    y_to_duplicate = y_encoded[indices_to_duplicate]\n",
    "    \n",
    "    X_fixed = pd.concat([X, X_to_duplicate], ignore_index=True)\n",
    "    y_fixed = pd.concat([pd.Series(y_encoded), pd.Series(y_to_duplicate)], ignore_index=True).values\n",
    "else:\n",
    "    print(\"No single-instance classes found that would break stratification.\")\n",
    "\n",
    "print(f\"Shape before fix: {X.shape}. Shape after fix: {X_fixed.shape}.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "    \n",
    "# --- 5. SPLIT THE DATA (CRITICAL LEAKAGE-PREVENTION STEP) ---\n",
    "# We split the 'fixed' data. X_fixed STILL CONTAINS THE TEXT/CATEGORICAL COLUMNS.\n",
    "print(\"\\n--- 5. Splitting Data into Training and Testing Sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fixed, y_fixed, \n",
    "    test_size=0.2,       # 20% for testing\n",
    "    random_state=42,     # For reproducibility\n",
    "    stratify=y_fixed     # Ensure class distribution is similar\n",
    ")\n",
    "print(\"Data successfully split.\")\n",
    "print(f\"Training set size: {len(X_train)} rows\")\n",
    "print(f\"Test set size: {len(X_test)} rows\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# --- 6. ONE-HOT ENCODE THE FEATURE COLUMNS (POST-SPLIT) ---\n",
    "# We learn the encoding schema ONLY from the training data to prevent data leakage.\n",
    "print(\"\\n--- 6. One-Hot Encoding Categorical Features ---\")\n",
    "# Automatically identify which columns are numeric and which are categorical from the training set\n",
    "numeric_features = X_train.select_dtypes(include='number').columns\n",
    "categorical_features = X_train.select_dtypes(include='object').columns\n",
    "\n",
    "print(f\"Identified Numeric Features: {list(numeric_features)}\")\n",
    "print(f\"Identified Categorical Features: {list(categorical_features)}\")\n",
    "\n",
    "# Create the encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# FIT the encoder ONLY on the TRAINING data's categorical columns\n",
    "encoder.fit(X_train[categorical_features])\n",
    "\n",
    "# TRANSFORM both the training and testing data's categorical columns\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_train[categorical_features]),\n",
    "    columns=encoder.get_feature_names_out(categorical_features),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_test[categorical_features]),\n",
    "    columns=encoder.get_feature_names_out(categorical_features),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Reset index on all parts to ensure clean concatenation\n",
    "X_train_numeric_reset = X_train[numeric_features].reset_index(drop=True)\n",
    "X_test_numeric_reset = X_test[numeric_features].reset_index(drop=True)\n",
    "X_train_encoded_reset = X_train_encoded.reset_index(drop=True)\n",
    "X_test_encoded_reset = X_test_encoded.reset_index(drop=True)\n",
    "\n",
    "# Combine encoded categorical columns with the original numeric columns\n",
    "X_train_processed = pd.concat([X_train_numeric_reset, X_train_encoded_reset], axis=1)\n",
    "X_test_processed = pd.concat([X_test_numeric_reset, X_test_encoded_reset], axis=1)\n",
    "\n",
    "print(\"\\nFeatures have been successfully encoded.\")\n",
    "print(f\"Shape of processed training features: {X_train_processed.shape}\")\n",
    "print(f\"Shape of processed testing features: {X_test_processed.shape}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "\n",
    "# STEP 2: DATA SPLITTING\n",
    "  ## Goal :Define class weights based on domain knowledge(Medical) to be fed into the data for use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Manually Defining Class Weights Based on Clinical Importance ---\")\n",
    "\n",
    "# First, let's create a mapping from class names to their encoded labels for clarity\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class Label Mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "# ===================================================================================\n",
    "# THIS IS WHERE YOU APPLY YOUR DOMAIN KNOWLEDGE.\n",
    "# You create the dictionary from scratch. Higher weight = higher penalty for getting it wrong.\n",
    "# You MUST provide a weight for every class label present in y_train.\n",
    "# ===================================================================================\n",
    "manual_weights_dict = {\n",
    "    label_mapping['REPEAT PAP SMEAR IN 3 YEARS AND FOR HPV VACCINE']: 1.0, # Majority/common class\n",
    "    label_mapping['FOR HPV VACCINE AND SEXUAL EDUCATION']: 1.2, # Slightly less common\n",
    "    label_mapping['FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY']: 20.0, # IMPORTANT - High penalty\n",
    "    label_mapping['FOR LASER THERAPY']: 30.0, # CRITICALLY IMPORTANT - Very high penalty\n",
    "    label_mapping['FOR COLPOSCOPY, BIOPSY, CYTOLOGY +/- TAH']: 25.0, # IMPORTANT - High penalty\n",
    "    label_mapping[\"REPEAT PAP SMEAR IN 3 YEARS\"]:10.0,\n",
    "    label_mapping[\"FOR PAP SMEAR\"]:5.0,\n",
    "    label_mapping[\"FOR ANNUAL FOLLOW-UP AND PAP SMEAR IN 3 YEARS\"]:20.0,\n",
    "    label_mapping[\"FOR HPV VACCINE, LIFESTYLE, AND SEXUAL EDUCATION\"]:1.5,\n",
    "    label_mapping[\"FOR COLPOSCOPY, CYTOLOGY, THEN LASER THERAPY\"] :30.0,\n",
    "    label_mapping[\"FOR REPEAT HPV TESTING ANNUALLY AND PAP SMEAR IN 3 YEARS\"]:25.0,\n",
    "    label_mapping['FOR COLPOSCOPY, BIOPSY, AND CYTOLOGY (TAH NOT RECOMMENDED)']: 20.0,\n",
    "    label_mapping['FOR BIOPSY AND CYTOLOGY (TAH NOT RECOMMENDED)']: 20.0\n",
    "}\n",
    "\n",
    "print(\"\\nCustom Weights Defined:\")\n",
    "print(manual_weights_dict)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDDBz22rCFSX"
   },
   "source": [
    "\n",
    "# STEP 3: Random Forest Classifier\n",
    "## Goal: Understand the features which have more influence on the target column. The columns with the most correlation will then be used to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Stage A: Calculate Feature Importance and Select Features (on training data) ---\n",
    "print(\"\\n--- A. Calculating Feature Importance on Training Data ONLY ---\")\n",
    "\n",
    "# We train a preliminary RandomForest model to get feature importances.\n",
    "# It's good practice to use the class weights here too so the importance calculation\n",
    "# is not biased towards features that are only useful for the majority class.\n",
    "feature_selector_model = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight=manual_weights_dict\n",
    ")\n",
    "feature_selector_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get the importance scores\n",
    "importances = feature_selector_model.feature_importances_\n",
    "feature_names = X_train_processed.columns\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Top 14 most important features (learned from training data):\")\n",
    "print(importance_df.head(14))\n",
    "\n",
    "# --- Stage B: Perform the Feature Selection ---\n",
    "print(\"\\n--- B. Selecting Features Based on Importance ---\")\n",
    "\n",
    "# We can use a threshold (e.g., select features with importance > 0.01)\n",
    "# Or we can use a helper like SelectFromModel to automatically select them.\n",
    "# 'threshold=\"median\"' will select all features with importance greater than the median.\n",
    "selector = SelectFromModel(feature_selector_model, threshold='median', prefit=True)\n",
    "\n",
    "# Use the selector to get the final versions of our datasets\n",
    "X_train_final = selector.transform(X_train_processed)\n",
    "X_test_final = selector.transform(X_test_processed) # Apply same transformation to test set\n",
    "\n",
    "print(f\"\\nOriginal number of features: {X_train_processed.shape[1]}\")\n",
    "print(f\"Number of features selected: {X_train_final.shape[1]}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "car15kHwLxB9"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
